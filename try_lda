#lda algorithm
import numpy as np
from scipy.special import digamma,polygamma,gamma

from math import exp,log

def find_theta(gamma,D,K):
    theta=np.empty((D,K),dtype=float)
    for d in range(D):
        denominator=np.sum(gamma[d])
        theta[d]=gamma[d]/denominator
    return theta

def find_word(document,n):
    word=1
    while sum(document[0:word])<n:
        word=word+1
    return word-1
def find_lowerbound(gamma1,phi,alpha,beta,document,N,K,V):
    gammasum=np.sum(gamma1)
    gamma_array=[x-digamma(gammasum) for x in digamma(gamma1)]
    l=0
    l+=log(gamma(np.sum(alpha)))
    l-=np.sum(np.log(gamma(alpha)))
    l+=np.sum(np.multiply([x-1 for x in alpha],gamma_array))
        
    for n in range(N):
        l+=np.sum(np.multiply(phi[n],gamma_array))
        
    for n in range(N):
        word=find_word(document,n)
        for j in range(V):
            if word!=j:
                continue
            for i in range(K):
                    l+=phi[n][i]*log(beta[i][j])
    l-=log(gamma(gammasum))
    l+=np.sum(np.log(gamma(gamma1)))
    l-=np.sum(np.multiply([x-1 for x in gamma1],gamma_array))
    l-=np.sum(np.multiply(phi,np.log(phi)))
    return l
class gp:
    """
    According to the paper, it uses varaitional inference instead of sampling.
    
    Find the value of gamma and eta given alpha and beta that minimize the KL divergence
    between the variational distribution and the true posterier distribution for a document.
    Use gamma to find theta.
    
    alpha is a 1*K vector
    beta is a K*V matrix
    alpha and beta are known here
    gamma is a 1*K vector
    theta is a 1*k vector following the Dirichlet distribution parametrized by gamma 
    phi is a N*k matrix 
    z[n] is a 1*k vector following the multinomial distribution parametrized by phi
    
    """
    def __init__(self,alpha,beta,document,K,N):
        self.alpha=alpha
        self.beta=beta
        self.document=document
        self.K=K
        self.N=N
        self.V=document.shape[0]
    def _init(self):
        phi=np.empty((self.N,self.K),dtype=float)
        gamma=np.empty(self.K,dtype=float)
        for i in range(self.N):
            for j in range(self.K):
                phi[i][j]=1/self.K
        self.phi=phi
        for i in range(self.K):
            gamma[i]=self.alpha[i]+self.N/self.K
        self.gamma=gamma
    def find_gp(self,tol=0.01):
        max_iter=2*self.N
        self._init()
        #lowerbound_history=[find_lowerbound(self.gamma,self.phi,self.alpha,self.beta,self.document,self.N,self.K,self.V)]
        for iteration in range(max_iter):
            phi_history=self.find_phi_history()
            gamma_history=self.find_gamma_history()
            self.phi_gamma()
           
            #lowerbound_history.append(find_lowerbound(self.gamma,self.phi,self.alpha,self.beta,self.document,self.N,self.K,self.V))
            #if iteration%5==0:
                #print(iteration,self.phi)
            #if np.abs(lowerbound_history[-2]-lowerbound_history[-1])<=tol:
                #print('gp converged with ll %.3f at iteration %d'%(self.lowerbound_history[-1],
                #                                                     iteration))
            #    break
            if np.linalg.norm(phi_history-self.phi)<=tol and np.linalg.norm(gamma_history-self.gamma)<=tol:
                break
    def phi_gamma(self):
        self.find_phi()
        self.find_gamma()  
    def find_phi(self):
        for n in range(self.N):
            v=find_word(self.document,n)
            for i in range(self.K):
                self.phi[n][i]=self.beta[i][v]*exp(digamma(self.gamma[i])-digamma(np.sum(self.gamma)))
            norm = np.sum(self.phi[n])
            self.phi[n]=self.phi[n]/norm
    def find_gamma(self):      
        self.gamma=self.alpha
        for n in range(self.N):
            self.gamma=np.add(self.gamma,self.phi[n])
    def find_phi_history(self):
        history=np.empty((self.N,self.K),dtype=float)
        for n in range(self.N):
            for i in range(self.K):
                history[n][i]=self.phi[n][i]
        return history
    def find_gamma_history(self):
        history= gamma=np.empty(self.K,dtype=float)
        for k in range(self.K):
            history[k]=self.gamma[k]
        return history
class lda():
    '''
    Find alpha and beta using gp defined above
    '''
    def __init__(self,topic_number):
        self.topic_number=topic_number
    def _init(self,Corpus):
        X=np.matrix(Corpus)
        D,V=X.shape
        self.dataset=Corpus
        self.D=D
        self.V=V
        self.N=np.empty(D,dtype=int)
        for d in range(D):
            self.N[d]=np.sum(X[d])
        self.phi=[]
        self.gamma=np.empty((self.D,self.topic_number),dtype=float)
        #self.alpha=np.empty(self.topic_number,dtype=float)
        np.random.seed(0)
        self.alpha=np.random.gamma(shape=2,scale=1,size=self.topic_number)
        np.random.seed(1)
        self.beta=np.random.dirichlet(np.ones(self.V),self.topic_number)
        norm=np.sum(self.beta)
        for i in range(self.topic_number):
            self.beta[i]/=norm
    
        
    def refresh_gp(self):
        self.phi=[]
        for d in range(self.D):
            document=self.dataset[d]
            dgp=gp(self.alpha,self.beta,document,self.topic_number,self.N[d])
            dgp.find_gp()
            self.phi.append(dgp.phi)
            self.gamma[d]=dgp.gamma
    def find_alpha(self):
        
        gradient=np.empty(self.topic_number,dtype=float)
        for i in range(self.topic_number):
            gradient[i]=self.D*(digamma(np.sum(self.alpha))-digamma(self.alpha[i]))
            for d in range(self.D):
                gradient[i]+=digamma(self.gamma[d][i])-digamma(np.sum(self.gamma[d]))
        h=np.empty(self.topic_number,dtype=float)
        denominator=0
        numerator=0
        z=self.D*polygamma(1,np.sum(self.alpha))
        for i in range(self.topic_number):
            h[i]=-self.D*polygamma(1,self.alpha[i])
            numerator+=gradient[i]/h[i]
            denominator+=1/h[i]
        denominator+=1/z
        c=numerator/denominator
        distance=0
        history=np.empty(self.topic_number,dtype=float)
        for i in range(self.topic_number):
            history[i]=self.alpha[i]
            self.alpha[i]-=(gradient[i]-c)/h[i]
        return history
    def find_beta(self):
        for i in range(self.topic_number):
            for j in range(self.V):
                num=0
                for d in range(self.D):
                    phi=self.phi[d]
                    document=self.dataset[d]
                    for n in range(self.N[d]):
                        word=find_word(document,n)
                        if word==j:
                            num=num+phi[n][i]
                self.beta[i][j]=num
            norm = np.sum(self.beta[i])
            self.beta[i]=self.beta[i]/norm
    def find_beta_history(self):
        history=np.empty((self.topic_number,self.V),dtype=float)
        for i in range(self.topic_number):
            for j in range(self.V):
                history[i][j]=self.beta[i][j]
        return history
    def find_corpusbound(self):
        bound=0
        for d in range(self.D):
            bound+=find_lowerbound(self.gamma[d],self.phi[d],self.alpha,self.beta,self.dataset[d],self.N[d],self.topic_number,self.V)
        return bound
    def fit(self,Corpus1,max_iter=50,tol=0.01):
        #np.seterr(divide = 'ignore') 
        self._init(Corpus1)
        alpha_history=np.empty(self.topic_number,dtype=float)
        self.refresh_gp()
        lowerbound_history=[self.find_corpusbound()]
        for iterations in range(max_iter):
            alpha_history=self.find_alpha()
            beta_history=self.find_beta_history()
            self.find_beta()
            #bound=self.find_corpusbound()
            #print(bound)
            #lowerbound_history.append(bound)
            #if abs(lowerbound_history[-2]-lowerbound_history[-1])<=tol:
            #    break
            if np.linalg.norm(alpha_history-self.alpha)<=tol and np.linalg.norm(beta_history-self.beta)<=tol:
                break
            self.refresh_gp()
           
        theta=find_theta(self.gamma,self.D,self.topic_number)   
        return theta
#for testing 
import pandas as pd
import re
import nltk
nltk.download('punkt')
from nltk.corpus import stopwords
nltk.download('stopwords')
from nltk.tokenize import word_tokenize
from stop_words import get_stop_words
from nltk.stem.porter import PorterStemmer
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.decomposition import LatentDirichletAllocation
doc_a = "Brocolli is good to eat. My brother likes to eat good brocolli, but not my mother."
doc_b = "My mother spends a lot of time driving my brother around to baseball practice."
doc_c = "Some health experts suggest that driving may cause increased tension and blood pressure."
doc_d = "I often feel pressure to perform well at school, but my mother never seems to drive my brother to do better."
doc_e = "Health professionals say that brocolli is good for your health."
doc_set = [doc_a, doc_b, doc_c, doc_d, doc_e]

stopwordlist=stopwords.words()
stemmer=PorterStemmer()
corpus=[]
for i in doc_set:
    
    doc=i.lower()
    tokens=word_tokenize(doc)
    stopped_tokens=[j for j in tokens if not j in stopwordlist]
    final_tokens=[stemmer.stem(j) for j in stopped_tokens]
    corpus.append(i)
cntVector=CountVectorizer(stop_words=stopwordlist)
words_frequency=cntVector.fit_transform(corpus)
a=lda(5)
result=a.fit(words_frequency.toarray())

print('My Lda result:\n')
print(result)
print('\nThe stadard result:\n')
lda2=LatentDirichletAllocation(n_components=5,learning_offset=50.,random_state=0)
print(lda2.fit_transform(words_frequency))

        
        
